# encoding: utf-8
import argparse
import os
import random
import logging
import torch
import json
import time
import csv
from typing import Any, Dict, List, Optional, Tuple

from src.framework import FewShotNERFramework
from dataloaders.spanner_dataset_msra import get_span_labels, get_loader
from src.bert_model_spanner import BertNER
from transformers import AutoTokenizer, AutoModel, AutoConfig
from src.config_spanner import BertNerConfig
from src.Evidential_woker import Span_Evidence
from metrics.mtrics_LinkResult import *
from args_config import get_args
from run_llm import *

os.environ['CUDA_LAUNCH_BLOCKING'] = '1'


def setup_seed(seed):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def get_logger(args, seed):
    path = os.path.join(args.logger_dir, f"{args.etrans_func}{seed}_{time.strftime('%m-%d_%H-%M-%S')}.txt")
    directory = os.path.dirname(path)
    if not os.path.exists(directory):
        os.makedirs(directory)

    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    formatter = logging.Formatter("%(asctime)s - %(levelname)s: %(message)s", datefmt='%Y-%m-%d %H:%M:%S')

    file_handler = logging.FileHandler(path)
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)

    stream_handler = logging.StreamHandler()
    stream_handler.setLevel(logging.DEBUG)
    stream_handler.setFormatter(formatter)

    # é˜²æ­¢é‡å¤ handler
    if not logger.handlers:
        logger.addHandler(file_handler)
        logger.addHandler(stream_handler)
    else:
        logger.handlers = []
        logger.addHandler(file_handler)
        logger.addHandler(stream_handler)

    return logger


# ---------- å†™å‡ºæ–‡ä»¶çš„å·¥å…·å‡½æ•° ----------

def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)


def write_metrics_csv(csv_path: str, rows: List[Dict[str, Any]]):
    """rows: list of dicts, keys will be header"""
    ensure_dir(os.path.dirname(csv_path))
    if not rows:
        # è‡³å°‘å†™ä¸€ä¸ªç©ºè¡¨å¤´ï¼Œé¿å…ç”¨æˆ·æ‰¾ä¸åˆ°æ–‡ä»¶
        rows = [{"epoch": 0, "train_loss": "", "train_p": "", "train_r": "", "train_f1": "",
                 "dev_p": "", "dev_r": "", "dev_f1": "", "test_p": "", "test_r": "", "test_f1": ""}]
    headers = list(rows[0].keys())
    with open(csv_path, "w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=headers)
        w.writeheader()
        for r in rows:
            w.writerow(r)


def save_best_model(model, path: str):
    ensure_dir(os.path.dirname(path))
    # ä¿å­˜æ•´ä¸ªæ¨¡å‹å¯¹è±¡ï¼Œä¾¿äºåç»­ç›´æ¥ torch.load ä½¿ç”¨
    torch.save(model, path)


def dump_test_details(framework: FewShotNERFramework, model, args, out_path: str):
    """
    å°½åŠ›ä» framework æ‹¿åˆ°æµ‹è¯•é›†å®Œæ•´è¯†åˆ«ç»“æœå¹¶å†™å‡ºã€‚
    å…¼å®¹å¤šç§å¯èƒ½çš„æ¥å£åç§°ï¼›è‹¥æ— æ³•è·å¾—ç»†èŠ‚ï¼Œå†™å…¥æç¤ºï¼Œè‡³å°‘ä¿è¯æ–‡ä»¶å­˜åœ¨ã€‚
    """
    ensure_dir(os.path.dirname(out_path))
    lines: List[str] = []
    wrote = False

    # 1) å¸¸è§ï¼šinference è¿”å› (metrics, details) / åªè¿”å› details
    try:
        ret = framework.inference(model)
        if isinstance(ret, tuple) and len(ret) == 2:
            _, details = ret
            if isinstance(details, list):
                lines = [str(x) for x in details]
                wrote = True
        elif isinstance(ret, list):
            lines = [str(x) for x in ret]
            wrote = True
    except Exception:
        pass

    # 2) å°è¯• get_last_test_details
    if not wrote:
        try:
            details = getattr(framework, "get_last_test_details", None)
            if callable(details):
                dl = details()
                if isinstance(dl, list):
                    lines = [str(x) for x in dl]
                    wrote = True
        except Exception:
            pass

    # 3) å…œåº•ï¼šæç¤º
    if not wrote:
        lines = [
            "No detailed prediction list could be retrieved from framework.",
            "Please expose a method like `framework.inference(model)` -> (metrics, details:list[str])",
            "or `framework.get_last_test_details()` returning a list of formatted prediction strings.",
        ]

    with open(out_path, "w", encoding="utf-8") as f:
        for ln in lines:
            f.write(ln.rstrip() + "\n")


def try_collect_epoch_history(framework: FewShotNERFramework) -> List[Dict[str, Any]]:
    """
    ä»æ¡†æ¶ä¸­å°½åŠ›æŠ“å–é€ epoch çš„å†å²æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰å°±å†™ csvï¼›æ²¡æœ‰å°±è¿”å›ç©ºï¼‰ã€‚
    å…¼å®¹å±æ€§åï¼šhistory / metrics_history / logs ç­‰ã€‚
    æœŸæœ›æ¯ä¸ªå…ƒç´ å«æœ‰ epoch/train/dev/test çš„ P/R/F1/Loss ç­‰é”®ã€‚
    """
    cand_attrs = ["history", "metrics_history", "logs", "epoch_logs"]
    for a in cand_attrs:
        if hasattr(framework, a):
            hist = getattr(framework, a)
            if isinstance(hist, list) and hist:
                # ç¡®ä¿æ˜¯ list[dict]
                if all(isinstance(x, dict) for x in hist):
                    return hist
    return []


def evaluate_test_metrics(framework: FewShotNERFramework, model) -> Tuple[float, Dict[str, Any]]:
    """
    è¿”å› (test_f1, test_metrics_dict)ã€‚å°½é‡å…¼å®¹ä¸åŒæ¥å£ã€‚
    """
    # ä¼˜å…ˆç”¨ inference
    try:
        ret = framework.inference(model)
        # å¸¸è§å‡ ç§è¿”å›
        if isinstance(ret, tuple):
            m = ret[0]
            if isinstance(m, dict):
                f1 = float(m.get("f1", m.get("F1", 0.0)))
                return f1, m
        elif isinstance(ret, dict):
            f1 = float(ret.get("f1", ret.get("F1", 0.0)))
            return f1, ret
    except Exception:
        pass

    # å°è¯• evaluate / evaluate_test
    for fn in ["evaluate", "evaluate_test", "eval_test", "test"]:
        try:
            method = getattr(framework, fn, None)
            if callable(method):
                m = method(model)
                if isinstance(m, dict):
                    f1 = float(m.get("f1", m.get("F1", 0.0)))
                    return f1, m
        except Exception:
            continue

    # å®åœ¨ä¸è¡Œå°±è¿”å› 0
    return 0.0, {"F1": 0.0}


def main():
    args = get_args()

    # å…è®¸é€šè¿‡å‘½ä»¤è¡Œå¯åŠ¨â€œå¤šéšæœºç§å­â€ä»¥ç¨³å®š F1ï¼ˆå¯é€‰ï¼‰
    auto_multi_seed = bool(getattr(args, "auto_multi_seed", False))
    num_seeds = int(getattr(args, "num_seeds", 1))
    num_seeds = max(1, num_seeds)

    if args.seed == -1:
        base_seed = random.randint(0, 100000000)
    else:
        base_seed = int(args.seed)

    print('random_int:', base_seed)
    print("Base Seed:", base_seed)

    logging.info(str(args.__dict__ if isinstance(args, argparse.ArgumentParser) else args))

    if args.use_combiner:
        from models.comb_voting import CombByVoting
        # âš ï¸ è¿™é‡ŒåŸæ¥æœ‰ `import os`ï¼Œå·²åˆ é™¤ï¼Œé¿å…æŠŠ os å˜æˆæœ¬å‡½æ•°çš„â€œå±€éƒ¨å˜é‡â€

        # ç»„åˆå™¨åŠŸèƒ½ï¼ˆä¿æŒä¸å˜ï¼‰
        dataname = args.dataname
        file_dir = args.data_dir
        fmodels = args.comb_model_results
        if not fmodels:
            print("é”™è¯¯ï¼šè¯·æŒ‡å®šå¾…ç»„åˆçš„æ¨¡å‹ç»“æœæ–‡ä»¶è·¯å¾„åˆ—è¡¨ (--comb_model_results)")
            return
        f1s = []
        for fmodel in fmodels:
            try:
                f1_str = fmodel.split('_')[-1].split('.')[0]
                f1s.append(float(f1_str) / 10000)
            except:
                f1s.append(0.8)
        cmodelname = "hybrid_combiner"
        if dataname == "conll03":
            classes = ["ORG", "PER", "LOC", "MISC"]
        elif dataname in ["msra", "weibo", "ontonotes4"]:
            classes = ["PER", "LOC", "ORG", "MISC"]
        elif dataname == "cluener":
            classes = ["address", "book", "company", "game", "government", "movie", "name", "organization", "position", "scene"]
        elif dataname == "cmeee":
            classes = ["dis", "sym", "pro", "equ", "dru", "ite", "bod"]
        else:
            classes = ["ORG", "PER", "LOC", "MISC"]
        fn_stand_res = fmodels[0] if fmodels else None
        prob_candidate = os.path.join(file_dir, f'{dataname}_spanner_prob.pkl')
        fn_prob = prob_candidate if os.path.exists(prob_candidate) else ''
        model_prob_files = getattr(args, 'comb_prob_files', None)
        if model_prob_files and len(model_prob_files) != len(fmodels):
            print('è­¦å‘Šï¼šcomb_prob_filesæ•°é‡ä¸comb_model_resultsä¸ä¸€è‡´ï¼Œå°†å¿½ç•¥ comb_prob_files')
            model_prob_files = None
        os.makedirs("comb_result", exist_ok=True)
        wscore = float(getattr(args, 'comb_wscore', 0.8))
        wf1 = float(getattr(args, 'comb_wf1', 1.0))
        combiner = CombByVoting(
            dataname, file_dir, fmodels, f1s, cmodelname, classes,
            fn_stand_res, fn_prob, wscore=wscore, wf1=wf1, model_probs=model_prob_files,
        )
        bio_fmodels, span_fmodels = [], []
        for fmodel in fmodels:
            if fmodel.endswith('_span.txt'):
                span_fmodels.append(fmodel)
                bio_file = fmodel.replace('_span.txt', '.txt')
                bio_path = os.path.join(file_dir, bio_file)
                if os.path.exists(bio_path):
                    bio_fmodels.append(bio_file)
                else:
                    print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ° {bio_file}ï¼Œå‰ä¸‰ç§ç­–ç•¥å°†è·³è¿‡æ­¤æ¨¡å‹")
            else:
                bio_fmodels.append(fmodel)
                span_file = fmodel.replace('.txt', '_span.txt')
                span_path = os.path.join(file_dir, span_file)
                if os.path.exists(span_path):
                    span_fmodels.append(span_file)
                else:
                    print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ° {span_file}ï¼Œspanç­–ç•¥å°†è·³è¿‡æ­¤æ¨¡å‹")
        strategies = args.comb_strategy
        if strategies == 'all':
            strategies = ['majority', 'weighted_f1', 'weighted_cat', 'span_score']
        else:
            strategies = [strategies] if isinstance(strategies, str) else strategies
        results = {}
        if any(s in strategies for s in ['majority', 'weighted_f1', 'weighted_cat']):
            combiner_bio = CombByVoting(
                dataname, file_dir, bio_fmodels, f1s, cmodelname, classes,
                bio_fmodels[0] if bio_fmodels else None, "", wscore=wscore, wf1=wf1, model_probs=None,
            )
            if 'majority' in strategies:
                results['majority'] = combiner_bio.voting_majority()
            if 'weighted_f1' in strategies:
                results['weighted_f1'] = combiner_bio.voting_weightByOverallF1()
            if 'weighted_cat' in strategies:
                results['weighted_cat'] = combiner_bio.voting_weightByCategotyF1()
        if 'span_score' in strategies:
            combiner_span = CombByVoting(
                dataname, file_dir, span_fmodels, f1s, cmodelname, classes,
                span_fmodels[0] if span_fmodels else None, fn_prob, wscore=wscore, wf1=wf1,
                model_probs=model_prob_files,
            )
            results['span_score'] = combiner_span.voting_spanPred_onlyScore(model_score_fn=None)
        print("\n=== ç»„åˆç»“æœæ€»ç»“ ===")
        for method, result in results.items():
            print(f"{method} F1: {result[0]:.4f}")
        print("\nç»„åˆå™¨æ¨ç†å·²å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° comb_result/ ç›®å½•")
        return

    # -------- LLM åˆ†ç±»æ¨¡å¼ -------
    if args.state == 'llm_classify':
        dic = json.load(open(args.selectShot_dir)) if args.selectShot_dir and args.selectShot_dir != 'None' else None
        linkToLLM(args.input_file, args.save_file, dic, args)
        return

    # -------- NER è®­ç»ƒ/æ¨ç† -------
    args.label2idx_list, args.morph2idx_list = get_span_labels(args)
    num_labels = len(args.label2idx_list)
    args.n_class = num_labels

    # è®­ç»ƒ/è¯„ä¼°è¾“å‡ºç›®å½•
    out_dir = getattr(args, "output_dir", "./output")
    ensure_dir(out_dir)
    metrics_csv_path = os.path.join(out_dir, "metrics_per_epoch.csv")
    best_model_path = os.path.join(out_dir, "best_model.pkl")
    detailed_test_path = os.path.join(out_dir, "detailed_test_results.txt")

    best_overall_f1 = -1.0
    best_overall_model_blob = None

    # å¤šç§å­å¾ªç¯ï¼ˆè‹¥æœªå¼€å¯ï¼Œnum_seeds=1ï¼‰
    seeds_to_run = [base_seed + i for i in range(num_seeds)] if auto_multi_seed else [base_seed]

    for run_idx, seed_num in enumerate(seeds_to_run, 1):
        print(f"\n===== Run {run_idx}/{len(seeds_to_run)} with seed {seed_num} =====")
        setup_seed(seed_num)
        logger = get_logger(args, seed_num)

        bert_config = BertNerConfig.from_pretrained(
            args.bert_config_dir,
            hidden_dropout_prob=args.bert_dropout,
            attention_probs_dropout_prob=args.bert_dropout,
            model_dropout=args.model_dropout
        )
        model = BertNER.from_pretrained(args.bert_config_dir, config=bert_config, args=args)
        model.cuda()

        train_data_loader = get_loader(args, args.data_dir, "train", True)
        dev_data_loader = get_loader(args, args.data_dir, "dev", False)
        if args.test_mode == 'ori':
            test_data_loader = get_loader(args, args.data_dir, "test", False)
        elif args.test_mode == 'typos' and args.dataname == 'conll03':
            test_data_loader = get_loader(args, args.data_dir_typos, "test", False)
        elif args.test_mode == 'oov' and args.dataname == 'conll03':
            test_data_loader = get_loader(args, args.data_dir_oov, "test", False)
        elif args.test_mode == 'ood' and args.dataname == 'conll03':
            test_data_loader = get_loader(args, args.data_dir_ood, "test", False)
        else:
            raise Exception("Invalid dataname or test_mode! Please check")

        edl = Span_Evidence(args, num_labels)
        framework = FewShotNERFramework(
            args, logger, None,
            train_data_loader, dev_data_loader, test_data_loader,
            edl, seed_num, num_labels=num_labels
        )

        # ------- è®­ç»ƒ -------
        if args.state == 'train':
            logger.info("ğŸš€ Start training...")
            framework.train(model)
            logger.info("training is ended! ğŸ‰")

            # è®­ç»ƒå†å² â†’ CSVï¼ˆè‹¥æ¡†æ¶æš´éœ²ï¼‰
            epoch_history = try_collect_epoch_history(framework)
            if not epoch_history:
                test_f1, test_m = evaluate_test_metrics(framework, model)
                one = {"epoch": getattr(args, "epoch", ""),
                       "train_loss": "",
                       "train_p": "", "train_r": "", "train_f1": "",
                       "dev_p": test_m.get("dev_p", ""), "dev_r": test_m.get("dev_r", ""), "dev_f1": test_m.get("dev_f1", ""),
                       "test_p": test_m.get("p", test_m.get("precision", "")),
                       "test_r": test_m.get("r", test_m.get("recall", "")),
                       "test_f1": test_m.get("f1", test_m.get("F1", ""))}
                write_metrics_csv(metrics_csv_path, [one])
            else:
                write_metrics_csv(metrics_csv_path, epoch_history)

            # è¯„ä¼°æµ‹è¯•é›†å¹¶è®°å½•æœ€å¥½æ¨¡å‹ï¼ˆä»¥ test F1 ä¸ºå‡†ï¼‰
            test_f1, test_metrics = evaluate_test_metrics(framework, model)
            logger.info(f"[Run {run_idx}] Test F1 = {test_f1:.4f}")

            # ä¿å­˜è¯¦ç»†æµ‹è¯•è¾“å‡º
            dump_test_details(framework, model, args, detailed_test_path)

            # è®°å½•æœ€å¥½æ¨¡å‹ï¼ˆåŸºäº test F1ï¼‰
            if test_f1 > best_overall_f1:
                best_overall_f1 = test_f1
                best_overall_model_blob = model  # ç›´æ¥å¼•ç”¨ï¼Œæœ€åç»Ÿä¸€ä¿å­˜

        # ------- ä»…æ¨ç† -------
        if args.state == 'inference':
            logger.info("ğŸš€ Start inference...")
            model = torch.load(args.inference_model)
            test_f1, test_metrics = evaluate_test_metrics(framework, model)
            logger.info(f"Inference Test F1 = {test_f1:.4f}")
            dump_test_details(framework, model, args, detailed_test_path)
            one = {"epoch": "",
                   "train_loss": "",
                   "train_p": "", "train_r": "", "train_f1": "",
                   "dev_p": test_metrics.get("dev_p", ""), "dev_r": test_metrics.get("dev_r", ""), "dev_f1": test_metrics.get("dev_f1", ""),
                   "test_p": test_metrics.get("p", test_metrics.get("precision", "")),
                   "test_r": test_metrics.get("r", test_metrics.get("recall", "")),
                   "test_f1": test_metrics.get("f1", test_metrics.get("F1", ""))}
            write_metrics_csv(metrics_csv_path, [one])
            if test_f1 > best_overall_f1:
                best_overall_f1 = test_f1
                best_overall_model_blob = model

    # ç»Ÿä¸€è½ç›˜ best_model.pkl
    if best_overall_model_blob is not None:
        save_best_model(best_overall_model_blob, best_model_path)
        print(f"[âœ“] Saved best model to: {best_model_path} (best test F1 = {best_overall_f1:.4f})")
    else:
        print("[!] No model to save as best_model.pkl (check training/inference state).")


if __name__ == '__main__':
    main()
